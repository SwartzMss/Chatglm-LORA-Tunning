python finetune.py --dataset_path data/alpaca --lora_rank 32 --per_device_train_batch_size 248 --per_device_eval_batch_size 1 --gradient_accumulation_steps 128 --save_steps 500 --save_total_limit 2 --learning_rate 1e-4 --remove_unused_columns false --logging_steps 50 --output_dir output
pause